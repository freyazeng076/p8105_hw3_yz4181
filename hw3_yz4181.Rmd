---
title: "hw3_yz4181"
author: "Yuanyuan Zeng(yz4181)"
date: "10/12/2021"
output: github_document
---

```{r, echo=FALSE}
library(tidyverse)
library(p8105.datasets)
library(lubridate)
library(ggplot2)
```

## Problem 1

```{r, message = FALSE}
data("instacart")

# find the number of aisle and aisle that the most items ordered from
aisles_df =
  instacart %>% 
  group_by(aisle) %>% 
  summarize(order_times = n()) %>% 
  arrange(desc(order_times))

nrow(aisles_df)
head(aisles_df, 1)
```
In the aisles_df, there are 134 rows and 3 variables which are aisle_id, aisle, and order_times. From this table, we know that there are 134 aisles and the aisle 83 which is corresponding to fresh vegetables is being ordered the most. 

```{r}
# making plot showing number of items order in each aisle
aisles_df %>% 
  filter(
    order_times > 10000) %>% 
  mutate(
    aisle = factor(aisle), 
    aisle = fct_reorder(aisle, order_times)
  ) %>% 
  ggplot(aes(x = aisle, y = order_times)) +
  geom_point(alpha = .5) +
  labs(
    title = "Figure1. Number of items ordered from each aisle",
    x = "Aisle",
    y = "Number of items ordered") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```
From the plot, we notice that there are two aisle which are being ordered most. 

```{r}
# Table showing most popular items in three aisles
most_popular_item = 
  instacart %>% 
  filter(
    aisle %in% c("baking ingredients", 
                 "dog food care", 
                 "packaged vegetables fruits")) %>% 
  group_by(aisle,product_name) %>% 
  summarize(number_of_order_times = n()) %>% 
  mutate(
    order_ranking = min_rank(desc(number_of_order_times))
  ) %>% 
  filter(order_ranking < 4) %>% 
  select(-order_ranking)
  
most_popular_item
```
The data frame of most popular items have 9 rows and 3 variables which are aisle, product_name, and number of order times. The most popular items in "baking ingredients" are 'Cane Sugar', 'Light Brown Sugar', and 'Pure Baking Soda'; in "dog food care" are 'Organix Chicken & Brown Rice Recipe', 'Small Dog Biscuits', and 'Snack Sticks Chicken & Rice Recipe Dog Treats'; in "packaged vegetables fruits" are 'Organic Baby Spinach', 'Organic Blueberries', and 'Organic Raspberries'.


```{r}
# Table showing mean hour of the day
mean_hour_of_the_day =
  instacart %>% 
  filter(
    product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  select(
    product_name,
    order_dow,
    order_hour_of_day) %>% 
  mutate(
    order_dow = wday(order_dow + 1, label = TRUE)) %>% 
  group_by(
    product_name,
    order_dow) %>% 
  summarize(
    mean_order_hour_of_day = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_order_hour_of_day
  )

mean_hour_of_the_day
```

The table of mean hour of the day includes 2 rows which are 'Pink Lady Apples' and 'Coffee Ice Cream' and 8 variables which are corresponding to seven days in a week. The mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week is calculated.


## Problem 2

```{r, message=FALSE}
data("brfss_smart2010")

# Clean the data
brfss =
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health",
         response %in% c("Excellent","Poor", "Very good", "Good", "Fair")) %>% 
  mutate(
    response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))
  )
```
After cleaning the data set, there are 23 variables and 10625 rows. The table only include responses from "Poor" to "Excellent".

```{r}
# Find the states which were observed at 7 or more locations in 2002
locations_2002 =
  brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs >=7)

nrow(locations_2002)
pull(locations_2002,locationabbr)
```
* There are 36 states that were observed at 7 or more locations in 2002 and the names of locations are listed above.

```{r}
# Find the states which were observed at 7 or more locations in 2010
locations_2010 = 
  brfss %>% 
  filter(year == 2010) %>% 
  group_by(locationabbr) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs >=7)

nrow(locations_2010)
pull(locations_2010,locationabbr)
```
* There are 45 states that were observed at 7 or more location in 2010 and the names of locations are listed above

```{r}
# Construct a data set and make a "spaghetti" plot
excellent_response = 
  brfss %>% 
  filter(response == "Excellent") %>% 
  select(year, locationabbr, data_value) %>% 
  group_by(year, locationabbr) %>% 
  summarize(average_data_value = mean(data_value, na.rm = T))

excellent_response %>% 
  ggplot(aes(x = year, y = average_data_value, color = locationabbr))+
  geom_line() +
  labs(
    title = "Figure 2. Average value over time within a state",
    x = "Year",
    y = "Average data value"
  ) 
```
Figure 2 show the average value over time within a state. We notice that most of average data value fall between 20 to 25. 

```{r}
# Make two-panel plot
brfss %>%
  filter(locationabbr == "NY",
         year %in% c(2006,2010)) %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_point() + 
  geom_boxplot() +
  facet_grid(.~year)
```
In 2006, the mean value of data value is increasing from "Poor" to "Very good". The mean value of data value for "Excellent" is less than that for "Very good". The distribution of data value for "Good" is most widespread among all responses, In 2010, the mean value of data value is also increasing from "Poor" to "Very good". 

## Problem 3

```{r}
accel_df = 
  read_csv("accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    weekday_weekend = 
      ifelse(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "weekday",
      ifelse(day %in% c("Saturday","Sunday"), "weekend", NA))) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "time",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(
    time = as.numeric(time)
  )

```
The original data records the activity in 5 weeks which are 35 days. In each day, the activity data is recorded 1440 times. After tidying the data, we aggregate all variables starting with "activity_" into one column "time". Now, the data set includes 6 variable: week, day_id, day, weekday_weekend, time, and activity_count.

```{r}
#showing total activity for each day
total_activity =
  accel_df %>% 
  group_by(week, day_id) %>% 
  summarize(
    total_acti = sum(activity_count)
  )

ggplot(total_activity, aes(x = day_id, y = total_acti)) +
  geom_point() +
  geom_line()+
  labs(
    x = "Day",
    y = "Total Activity in a Day",
    title = "Figure3. Total activity in Each Day"
  ) +
  scale_y_continuous(
    breaks = c(0,1E05, 2E05, 3E05, 4E05, 5E05, 6E05, 7E05),
  )
```
Figure 3 shows the total activity in each day over 5 weeks. There is no apparent trend over the weeks. Most of the total activity in a day fall between 300000 and 500000.

```{r}
# showing the activity over the course of the day
accel_df %>% 
   ggplot(
    aes(x = time,
    y = activity_count, 
    group = day_id,
    color = day))+
  geom_smooth(aes(group = day)) + 
  scale_x_continuous(
    breaks = c(0,300, 600, 900, 1200, 1440),
    labels = c("0AM","5AM", "10AM","3PM","8PM","12AM"))
```

